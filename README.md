# GAN Anime Faces

## Overview
This project implements a Generative Adversarial Network (GAN) for generating anime faces. The architecture is based on DCGAN principles, and the implementation includes various techniques to improve training stability and evaluation metrics.

## Table of Contents
- [Installation](#installation)
- [Usage](#usage)
- [GAN Architecture](#gan-architecture)
- [Training Process](#training-process)
- [Evaluation Results](#evaluation-results)
- [License](#license)

## Installation
To run this project, ensure you have Python 3.6 or higher installed. You will also need to install the required packages. You can do this using pip:

```bash
pip install torch torchvision matplotlib numpy scipy scikit-learn
```

## Usage
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd gan-anime-faces
   ```

2. Prepare your dataset of anime faces and place it in the `anime_dataset` directory.

3. Run the GAN training script:
   ```bash
   python gan_anime_faces.py
   ```

4. The generated images will be saved in the `generated_images` directory, and training history will be saved in the `output` directory.

## GAN Architecture
The GAN consists of two main components:
- **Generator**: This model takes random noise as input and generates images that resemble anime faces. It uses transposed convolutional layers to upsample the input noise into a full-sized image.
- **Discriminator**: This model takes an image as input and predicts whether it is real (from the dataset) or fake (generated by the generator). It uses convolutional layers to downsample the input image.

## Training Process
The training process involves alternating between training the discriminator and the generator:
1. The discriminator is trained on real images with labels set to 1 and on fake images with labels set to 0.
2. The generator is trained to produce images that the discriminator classifies as real.
3. Label smoothing and noise are applied to improve training stability.

## Evaluation Results
The model's performance can be evaluated using metrics such as Frechet Inception Distance (FID) and Inception Score (IS). These metrics help assess the quality of the generated images compared to real images.

## License
This project is licensed under the MIT License. See the LICENSE file for details.